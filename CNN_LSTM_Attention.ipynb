{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjain2/csci544-group32/blob/main/CNN_LSTM_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cwr1o7QqTZW3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Flatten, Dense, Input, Conv1D, LSTM, Concatenate\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import gc\n",
        "import pickle\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5M6smDTbLN",
        "outputId": "ff502841-3c3a-4409-8b7e-666f9a1e7475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# with open('/content/gdrive/MyDrive/ProjectEmbeddings/test_tok_emb.pkl', 'rb') as f:\n",
        "#     train_tok_emb = pickle.load(f)\n",
        "\n",
        "# with open('/content/gdrive/MyDrive/ProjectEmbeddings/train_sent_emb.pkl', 'rb') as f:\n",
        "#     train_sent_emb = pickle.load(f)\n",
        "\n",
        "# # with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/test_sent_emb.pkl', 'rb') as f:\n",
        "# #     test_sent_emb = pickle.load(f)\n",
        "\n",
        "# with open('/content/gdrive/MyDrive/ProjectEmbeddings/test_tok_emb.pkl', 'rb') as f:\n",
        "#     test_tok_emb = pickle.load(f)\n",
        "\n",
        "# with open('/content/gdrive/MyDrive/ProjectEmbeddings/y_train.pkl', 'rb') as f:\n",
        "#     y_train = pickle.load(f)\n",
        "\n",
        "# with open('/content/gdrive/MyDrive/ProjectEmbeddings/y_test.pkl', 'rb') as f:\n",
        "#     y_test = pickle.load(f)\n",
        "\n",
        "# with open('df_train_folds.pkl', 'rb') as f:\n",
        "#     df_train_folds = pickle.load(f)"
      ],
      "metadata": {
        "id": "Kgyof1yKTcMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/train_tok_emb.pkl', 'rb') as f:\n",
        "    train_tok_emb = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/train_sent_emb.pkl', 'rb') as f:\n",
        "    train_sent_emb = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/test_sent_emb.pkl', 'rb') as f:\n",
        "    test_sent_emb = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/test_tok_emb.pkl', 'rb') as f:\n",
        "    test_tok_emb = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/y_train.pkl', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/y_test.pkl', 'rb') as f:\n",
        "    y_test = pickle.load(f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/NLP/Project/feedback-prize-english-language-learning/df_train_folds.pkl', 'rb') as f:\n",
        "    df_train_folds = pickle.load(f)"
      ],
      "metadata": {
        "id": "VvcysiJeFC12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer():\n",
        "  adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  return adam\n",
        "\n",
        "def mcrmse(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=0)"
      ],
      "metadata": {
        "id": "XviDHPbdTy6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        print(self.W.shape)\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\") \n",
        "        print(self.b.shape)       \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        temp_dot = K.dot(x,self.W)\n",
        "        temp_sum = temp_dot + self.b\n",
        "        temp_tanh = K.tanh(temp_sum)\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "metadata": {
        "id": "cqaFyDuu1s8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_CNN1D(feat_len, seq_len, embedding_len):\n",
        "    drop_lstm = 0.25\n",
        "    drop_dense = 0.25\n",
        "    num_lstm=150\n",
        "    input_1 = Input(shape=(seq_len, embedding_len,),name = 'input_comment_1')\n",
        "    conv_1_1 = Conv1D(64,3,strides=1, padding='same',activation='relu')(input_1)\n",
        "    lstm_1_1 = LSTM(64,dropout=drop_lstm,return_sequences=True,dtype=tf.float32)(conv_1_1)\n",
        "    concate_1 = keras.layers.Concatenate(axis=-1)([conv_1_1, lstm_1_1])\n",
        "    flatten_1 = Flatten()(lstm_1_1)\n",
        "    attention_output_1 = Flatten()(attention()(concate_1))\n",
        "\n",
        "    # creating layers for parent comment data\n",
        "    input_2 = Input(shape=(seq_len, embedding_len,),name = 'input_comment_2')\n",
        "    conv_1_2 = Conv1D(128,3,strides=1, padding='same',activation='relu')(input_2)\n",
        "    lstm_1_2 =LSTM(128,dropout=drop_lstm,return_sequences=True,dtype=tf.float32)(conv_1_2)\n",
        "    concate_2 = keras.layers.Concatenate(axis=-1)([conv_1_2, lstm_1_2])\n",
        "    flatten_2 = Flatten()(lstm_1_2)\n",
        "\n",
        "    attention_output_2 = Flatten()(attention()(concate_2))\n",
        "    \n",
        "    # creating further layers\n",
        "    input_dense = Input(shape=(feat_len,),name = 'input_comment')\n",
        "    dense_num_layer = Dense(128, activation = \"relu\")(input_dense)\n",
        "\n",
        "    #concatenated_layer = keras.layers.concatenate([Flatten()(concate_1),attention_output_1,Flatten()(concate_2),attention_output_2,dense_num_layer],axis=-1)\n",
        "    # concatenated_layer = keras.layers.concatenate([attention_output_1,flatten_1,attention_output_2,flatten_2,dense_num_layer],axis=-1)\n",
        "\n",
        "    concatenated_layer = keras.layers.concatenate([attention_output_1,attention_output_2, dense_num_layer],axis=-1)\n",
        "    \n",
        "    x = Dense(128, activation = 'relu',kernel_initializer = glorot_uniform(seed=42))(concatenated_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(64, activation = 'relu',kernel_initializer = glorot_uniform(seed=42))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(32, activation = 'relu',kernel_initializer = glorot_uniform(seed=42))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(16, activation = 'relu',kernel_initializer= glorot_uniform(seed=42))(x)\n",
        "    #x = Dropout(drop_dense)(x)\n",
        "    output = Dense(6,activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs = [input_dense, input_1, input_2], outputs = [output])\n",
        "    model.compile(optimizer=get_optimizer(), loss = mcrmse, metrics = mcrmse)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "jbipzuxUT0x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_mcrmse', factor = 0.25, patience = 2, verbose = 1)\n",
        "earlystop = EarlyStopping(monitor = 'val_mcrmse',  mode=\"min\",min_delta = 0, patience = 25,verbose = 1)\n",
        "\n",
        "callbacks = [reduce_lr,earlystop]"
      ],
      "metadata": {
        "id": "JdAQ2OReUFFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LSTM_CNN1D(train_sent_emb.shape[1], train_tok_emb.shape[1], train_tok_emb.shape[2])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt7UfZYJq703",
        "outputId": "2433ee0e-0a25-4512-b7ae-f5e5a90f8a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 1)\n",
            "(640, 1)\n",
            "(256, 1)\n",
            "(640, 1)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_comment_1 (InputLayer)   [(None, 640, 768)]   0           []                               \n",
            "                                                                                                  \n",
            " input_comment_2 (InputLayer)   [(None, 640, 768)]   0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 640, 64)      147520      ['input_comment_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 640, 128)     295040      ['input_comment_2[0][0]']        \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 640, 64)      33024       ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 640, 128)     131584      ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 640, 128)     0           ['conv1d[0][0]',                 \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 640, 256)     0           ['conv1d_1[0][0]',               \n",
            "                                                                  'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " attention (attention)          (None, 128)          768         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " attention_1 (attention)        (None, 256)          896         ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " input_comment (InputLayer)     [(None, 768)]        0           []                               \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 128)          0           ['attention[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 256)          0           ['attention_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          98432       ['input_comment[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 512)          0           ['flatten_1[0][0]',              \n",
            "                                                                  'flatten_3[0][0]',              \n",
            "                                                                  'dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          65664       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128)         512         ['dense_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           8256        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 64)          256         ['dense_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 32)           2080        ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32)          128         ['dense_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 16)           528         ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 6)            102         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 784,790\n",
            "Trainable params: 784,342\n",
            "Non-trainable params: 448\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "preds = []\n",
        "scores = []\n",
        "def comp_score(y_true,y_pred):\n",
        "    rmse_scores = []\n",
        "    for i in range(len(target_cols)):\n",
        "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
        "    return np.mean(rmse_scores)\n",
        "\n",
        "def train(all_train_text_feats, all_train_token_embedding, dftr, FOLDS=2):\n",
        "  #for fold in tqdm(range(FOLDS),total=FOLDS):\n",
        "  for fold in range(FOLDS):\n",
        "      print('#'*25)\n",
        "      print('### Fold',fold+1)\n",
        "      print('#'*25)\n",
        "      \n",
        "      dftr_ = dftr[dftr[\"FOLD\"]!=fold]\n",
        "      dfev_ = dftr[dftr[\"FOLD\"]==fold]\n",
        "      \n",
        "      tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
        "      tr_token_feats = all_train_token_embedding[list(dftr_.index),:]\n",
        "      y_train = dftr_.loc[list(dftr_.index), target_cols].values\n",
        "      \n",
        "      ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
        "      ev_token_feats = all_train_token_embedding[list(dfev_.index),:]\n",
        "      y_test = dfev_.loc[list(dfev_.index), target_cols].values\n",
        "      \n",
        "      ev_preds = np.zeros((len(ev_text_feats),6))\n",
        "      # test_preds = np.zeros((len(te_text_feats),6))\n",
        "      \n",
        "      hitory=model.fit(x=[tr_text_feats, tr_token_feats, tr_token_feats],\n",
        "                  y=y_train,\n",
        "                  epochs=15,\n",
        "                  batch_size=32,\n",
        "                  validation_data=([ev_text_feats, ev_token_feats, ev_token_feats], y_test),\n",
        "                  callbacks=callbacks)\n",
        "      \n",
        "      ev_preds = model.predict([ev_text_feats, ev_token_feats, ev_token_feats])\n",
        "      # test_preds = model.predict([te_text_feats, te_token_embeddings, te_token_embeddings])\n",
        "      print()\n",
        "      score = comp_score(dfev_[target_cols].values,ev_preds)\n",
        "      scores.append(score)\n",
        "      print(\"Fold : {} RSME score: {}\".format(fold+1,score))\n",
        "      # preds.append(test_preds)\n",
        "      del dftr_\n",
        "      del dfev_ \n",
        "      del tr_text_feats \n",
        "      del tr_token_feats \n",
        "      del y_train\n",
        "      del ev_text_feats \n",
        "      del ev_token_feats \n",
        "      del y_test\n",
        "      \n",
        "      gc.collect()\n",
        "      \n",
        "  print('#'*25)\n",
        "  print('Overall CV RSME =',np.mean(scores))\n",
        "  return model"
      ],
      "metadata": {
        "id": "2o5i5jGXqzCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(train_sent_emb, train_tok_emb, df_train_folds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqBwzYao4WyE",
        "outputId": "1c4248fc-1ad0-4512-c382-ab472b32312b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#########################\n",
            "### Fold 1\n",
            "#########################\n",
            "Epoch 1/15\n",
            "79/79 [==============================] - 26s 139ms/step - loss: 2.2950 - mcrmse: 2.2885 - val_loss: 2.2755 - val_mcrmse: 2.2735 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.7717 - mcrmse: 0.7693 - val_loss: 0.9578 - val_mcrmse: 0.9578 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.5556 - mcrmse: 0.5553 - val_loss: 0.8490 - val_mcrmse: 0.8467 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.5378 - mcrmse: 0.5378 - val_loss: 0.6483 - val_mcrmse: 0.6464 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.5201 - mcrmse: 0.5224 - val_loss: 0.5751 - val_mcrmse: 0.5723 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.5197 - mcrmse: 0.5240 - val_loss: 0.5210 - val_mcrmse: 0.5191 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.5121 - mcrmse: 0.5115 - val_loss: 0.5807 - val_mcrmse: 0.5783 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.5207 - mcrmse: 0.5217\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.5207 - mcrmse: 0.5217 - val_loss: 0.7098 - val_mcrmse: 0.7067 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4806 - mcrmse: 0.4868 - val_loss: 0.4819 - val_mcrmse: 0.4800 - lr: 2.5000e-04\n",
            "Epoch 10/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4698 - mcrmse: 0.4713 - val_loss: 0.4945 - val_mcrmse: 0.4919 - lr: 2.5000e-04\n",
            "Epoch 11/15\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.4764 - mcrmse: 0.4775 - val_loss: 0.4798 - val_mcrmse: 0.4790 - lr: 2.5000e-04\n",
            "Epoch 12/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4706 - mcrmse: 0.4718 - val_loss: 0.4895 - val_mcrmse: 0.4881 - lr: 2.5000e-04\n",
            "Epoch 13/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4701 - mcrmse: 0.4781\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "79/79 [==============================] - 8s 101ms/step - loss: 0.4701 - mcrmse: 0.4781 - val_loss: 0.5063 - val_mcrmse: 0.5045 - lr: 2.5000e-04\n",
            "Epoch 14/15\n",
            "79/79 [==============================] - 8s 102ms/step - loss: 0.4643 - mcrmse: 0.4672 - val_loss: 0.4766 - val_mcrmse: 0.4750 - lr: 6.2500e-05\n",
            "Epoch 15/15\n",
            "79/79 [==============================] - 8s 101ms/step - loss: 0.4594 - mcrmse: 0.4642 - val_loss: 0.4787 - val_mcrmse: 0.4771 - lr: 6.2500e-05\n",
            "20/20 [==============================] - 2s 51ms/step\n",
            "\n",
            "Fold : 1 RSME score: 0.483232738343571\n",
            "#########################\n",
            "### Fold 2\n",
            "#########################\n",
            "Epoch 1/15\n",
            "79/79 [==============================] - 9s 114ms/step - loss: 0.4691 - mcrmse: 0.4687 - val_loss: 0.4261 - val_mcrmse: 0.4266 - lr: 6.2500e-05\n",
            "Epoch 2/15\n",
            "79/79 [==============================] - 8s 101ms/step - loss: 0.4612 - mcrmse: 0.4629 - val_loss: 0.4277 - val_mcrmse: 0.4285 - lr: 6.2500e-05\n",
            "Epoch 3/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4631 - mcrmse: 0.4627\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "79/79 [==============================] - 8s 102ms/step - loss: 0.4631 - mcrmse: 0.4627 - val_loss: 0.4279 - val_mcrmse: 0.4286 - lr: 6.2500e-05\n",
            "Epoch 4/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4598 - mcrmse: 0.4588 - val_loss: 0.4255 - val_mcrmse: 0.4261 - lr: 1.5625e-05\n",
            "Epoch 5/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4688 - mcrmse: 0.4708 - val_loss: 0.4276 - val_mcrmse: 0.4284 - lr: 1.5625e-05\n",
            "Epoch 6/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4581 - mcrmse: 0.4583\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4581 - mcrmse: 0.4583 - val_loss: 0.4292 - val_mcrmse: 0.4300 - lr: 1.5625e-05\n",
            "Epoch 7/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4614 - mcrmse: 0.4609 - val_loss: 0.4289 - val_mcrmse: 0.4296 - lr: 3.9063e-06\n",
            "Epoch 8/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4597 - mcrmse: 0.4593\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4597 - mcrmse: 0.4593 - val_loss: 0.4295 - val_mcrmse: 0.4301 - lr: 3.9063e-06\n",
            "Epoch 9/15\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4608 - mcrmse: 0.4608 - val_loss: 0.4286 - val_mcrmse: 0.4292 - lr: 9.7656e-07\n",
            "Epoch 10/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4647 - mcrmse: 0.4684\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4647 - mcrmse: 0.4684 - val_loss: 0.4292 - val_mcrmse: 0.4298 - lr: 9.7656e-07\n",
            "Epoch 11/15\n",
            "79/79 [==============================] - 8s 98ms/step - loss: 0.4615 - mcrmse: 0.4611 - val_loss: 0.4301 - val_mcrmse: 0.4306 - lr: 2.4414e-07\n",
            "Epoch 12/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4602 - mcrmse: 0.4600\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "79/79 [==============================] - 8s 101ms/step - loss: 0.4602 - mcrmse: 0.4600 - val_loss: 0.4295 - val_mcrmse: 0.4301 - lr: 2.4414e-07\n",
            "Epoch 13/15\n",
            "79/79 [==============================] - 8s 103ms/step - loss: 0.4618 - mcrmse: 0.4632 - val_loss: 0.4302 - val_mcrmse: 0.4308 - lr: 6.1035e-08\n",
            "Epoch 14/15\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4581 - mcrmse: 0.4578\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "79/79 [==============================] - 8s 100ms/step - loss: 0.4581 - mcrmse: 0.4578 - val_loss: 0.4290 - val_mcrmse: 0.4296 - lr: 6.1035e-08\n",
            "Epoch 15/15\n",
            "79/79 [==============================] - 8s 99ms/step - loss: 0.4541 - mcrmse: 0.4555 - val_loss: 0.4284 - val_mcrmse: 0.4290 - lr: 1.5259e-08\n",
            "20/20 [==============================] - 1s 51ms/step\n",
            "\n",
            "Fold : 2 RSME score: 0.4329838426980665\n",
            "#########################\n",
            "Overall CV RSME = 0.45810829052081875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict([test_sent_emb, test_tok_emb, test_tok_emb])\n",
        "print(comp_score(np.array(y_test), preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcsK0I3jdFAr",
        "outputId": "494acaaa-826b-455e-e645-b44b3e111eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 1s 53ms/step\n",
            "0.4821823667907146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PscLK5DMG_Vj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}